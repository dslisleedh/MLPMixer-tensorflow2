{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55dd954c-34b4-49c0-b7e7-f40cb1cce93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150bd358-851c-484f-a3cb-7beb73cafe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:\\datasets\\img_align_celeba\\img_align_celeba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1d743c-2e0f-432f-baae-748506827842",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = pd.read_csv('C:/datasets/list_attr_celeba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfca283f-2a2b-433e-83ea-a41d05b14a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = tag[['image_id','Male']]\n",
    "tag.Male = np.where(tag.Male == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bbf535d-ad5b-4921-928e-cf2525021f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93241</th>\n",
       "      <td>093242.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121413</th>\n",
       "      <td>121414.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10624</th>\n",
       "      <td>010625.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82523</th>\n",
       "      <td>082524.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73192</th>\n",
       "      <td>073193.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36890</th>\n",
       "      <td>036891.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>001993.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47852</th>\n",
       "      <td>047853.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79821</th>\n",
       "      <td>079822.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59328</th>\n",
       "      <td>059329.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162079 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  Male\n",
       "93241   093242.jpg     1\n",
       "121413  121414.jpg     0\n",
       "10624   010625.jpg     1\n",
       "82523   082524.jpg     0\n",
       "73192   073193.jpg     0\n",
       "...            ...   ...\n",
       "36890   036891.jpg     1\n",
       "1992    001993.jpg     0\n",
       "47852   047853.jpg     0\n",
       "79821   079822.jpg     0\n",
       "59328   059329.jpg     0\n",
       "\n",
       "[162079 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = tag.sample(frac = .8, random_state=42)\n",
    "test = tag.drop(train.index)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d59c41-e93a-4beb-9eeb-b5f34a79bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa001c9d-79ae-4ff0-ac9d-7daf3435d1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121560 validated image filenames.\n",
      "Found 40519 validated image filenames.\n",
      "Found 40520 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "directory = directory,\n",
    "x_col=\"image_id\",\n",
    "y_col=\"Male\",\n",
    "subset=\"training\",\n",
    "batch_size=16,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "    class_mode='raw',\n",
    "target_size=(256,256))\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "directory=directory,\n",
    "x_col=\"image_id\",\n",
    "y_col=\"Male\",\n",
    "subset=\"validation\",\n",
    "batch_size=16,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\",\n",
    "target_size=(256,256))\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=test,\n",
    "directory=directory,\n",
    "x_col=\"image_id\",\n",
    "y_col='Male',\n",
    "batch_size=16,\n",
    "seed=42,\n",
    "shuffle=False,\n",
    "class_mode='raw',\n",
    "target_size=(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7af2fb0-aa2b-46e0-83a1-31e6e8edc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, row_size, kernel_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.row_size = row_size\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(self.kernel_size, 1, input_shape = (self.row_size, self.kernel_size))\n",
    "        self.conv2 = tf.keras.layers.Conv1D(self.kernel_size, 1)\n",
    "        \n",
    "    def call(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = tf.nn.gelu(X)\n",
    "        X = self.conv2(X)\n",
    "        return X\n",
    "\n",
    "class Mixer_block(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_patches, hidden_chaneel_size):\n",
    "        super(Mixer_block, self).__init__()\n",
    "        self.n_patches = n_patches\n",
    "        self.hidden_chaneel_size = hidden_chaneel_size\n",
    "        \n",
    "        self.ln = tf.keras.layers.LayerNormalization(axis = 1)\n",
    "        self.t = tf.keras.layers.Permute((2,1))\n",
    "        self.mlp1 = MLP(self.hidden_chaneel_size, self.n_patches)\n",
    "        self.mlp2 = MLP(self.n_patches, self.hidden_chaneel_size)\n",
    "        \n",
    "    def call(self, X):\n",
    "        skip_X = X\n",
    "        X = self.ln(X)\n",
    "        X = self.t(X)\n",
    "        X = self.mlp1(X)\n",
    "        X = self.t(X)\n",
    "        X = X  + skip_X\n",
    "        skip_X2 = X\n",
    "        X = self.ln(X)\n",
    "        X = self.mlp2(X)\n",
    "        X = X + skip_X2\n",
    "        return X\n",
    "\n",
    "class MLP_Mixer(tf.keras.models.Model):\n",
    "    def __init__(self, patch_size:int, image_size:int, hidden_channel_size:int, n_mixers:int, fc_nodes:int, fc_layers:int):\n",
    "        tf.random.set_seed(42)\n",
    "        super(MLP_Mixer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        if (image_size % patch_size) != 0:\n",
    "            raise ValueError('size error')\n",
    "        self.n_patches = int((tf.square(self.image_size) / tf.square(self.patch_size)).numpy())\n",
    "        self.hidden_channel_size = hidden_channel_size\n",
    "        self.n_mixers = n_mixers\n",
    "        self.fc_nodes = fc_nodes\n",
    "        self.fc_layers = fc_layers\n",
    "        \n",
    "        self.patchConv = tf.keras.layers.Conv2D(self.hidden_channel_size, (self.patch_size, self.patch_size), strides = (self.patch_size, self.patch_size))\n",
    "        self.reshapeL = tf.keras.layers.Reshape((self.n_patches, self.hidden_channel_size))\n",
    "        self.mixer_blocks = [Mixer_block(self.n_patches, self.hidden_channel_size) for _ in range(self.n_mixers)]\n",
    "        self.gap = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.fc = [tf.keras.layers.Dense(self.fc_nodes, activation = tfa.activations.gelu, kernel_initializer = tf.keras.initializers.lecun_normal(seed = i)) for i in range(self.fc_layers)]\n",
    "        self.classifier = tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "        \n",
    "    def call(self, X):\n",
    "        X = self.patchConv(X)\n",
    "        X = self.reshapeL(X)\n",
    "        for mixer in self.mixer_blocks:\n",
    "            X = mixer(X)\n",
    "        X = self.gap(X)\n",
    "        for fc_layer in self.fc:\n",
    "            X = fc_layer(X)\n",
    "        X = self.classifier(X)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce3fa57-277f-40f0-afa0-6816e637f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_mixer = MLP_Mixer(64, 256, 512, 5, 512, 2)\n",
    "opt = tf.keras.optimizers.Adam(.0002)\n",
    "mlp_mixer.compile(optimizer=opt, loss=tf.losses.binary_crossentropy, metrics='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "881533c4-4679-48f8-bf62-db37985bbfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7597/7597 [==============================] - 297s 39ms/step - loss: 0.2377 - acc: 0.8997 - val_loss: 0.1714 - val_acc: 0.9304\n",
      "Epoch 2/10\n",
      "7597/7597 [==============================] - 294s 39ms/step - loss: 0.1580 - acc: 0.9372 - val_loss: 0.1470 - val_acc: 0.9458\n",
      "Epoch 3/10\n",
      "7597/7597 [==============================] - 294s 39ms/step - loss: 0.1359 - acc: 0.9467 - val_loss: 0.1435 - val_acc: 0.9421\n",
      "Epoch 4/10\n",
      "7597/7597 [==============================] - 295s 39ms/step - loss: 0.1184 - acc: 0.9538 - val_loss: 0.1301 - val_acc: 0.9480\n",
      "Epoch 5/10\n",
      "7597/7597 [==============================] - 297s 39ms/step - loss: 0.1071 - acc: 0.9584 - val_loss: 0.1315 - val_acc: 0.9473\n",
      "Epoch 6/10\n",
      "7597/7597 [==============================] - 298s 39ms/step - loss: 0.0980 - acc: 0.9626 - val_loss: 0.1202 - val_acc: 0.9531\n",
      "Epoch 7/10\n",
      "7597/7597 [==============================] - 300s 39ms/step - loss: 0.0885 - acc: 0.9658 - val_loss: 0.1213 - val_acc: 0.9525\n",
      "Epoch 8/10\n",
      "7597/7597 [==============================] - 299s 39ms/step - loss: 0.0811 - acc: 0.9690 - val_loss: 0.1136 - val_acc: 0.9581\n",
      "Epoch 9/10\n",
      "7597/7597 [==============================] - 299s 39ms/step - loss: 0.0746 - acc: 0.9712 - val_loss: 0.1263 - val_acc: 0.9548\n",
      "Epoch 10/10\n",
      "7597/7597 [==============================] - 295s 39ms/step - loss: 0.0665 - acc: 0.9751 - val_loss: 0.1255 - val_acc: 0.9555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b9da9b5c40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "mlp_mixer.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610ec8e-bc0f-459b-9a3f-69d6a0a12c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
